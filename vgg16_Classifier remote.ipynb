{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGJXvLUiYLuL",
        "outputId": "4eb79332-ecab-4841-d744-965607f8e56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Mar 28 22:44:02 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 531.41                 Driver Version: 531.41       CUDA Version: 12.1     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650 Ti    WDDM | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   50C    P8                5W /  N/A|    386MiB /  4096MiB |      2%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      6888    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      7308    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     15108    C+G   ...on\\111.0.1661.54\\msedgewebview2.exe    N/A      |\n",
            "|    0   N/A  N/A     19428    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     22420    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
            "|    0   N/A  N/A     29332    C+G   ...x64__qmba6cd70vzyy\\ArmouryCrate.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6icOTaoetHV",
        "outputId": "932ba49e-8cad-4d8c-8617-cadb58c82716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.11.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGo_7V0ye8h1"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "kcG-2aGne7mn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input,Lambda,Dense,Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "At7SSQfxlzLp"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH=r'C:\\Users\\rctuh\\Desktop\\minor projects\\Vgg16_Image_Classifier\\cats_and_dogs\\cats_and_dogs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjzCwbZUmEUp",
        "outputId": "4f16db6c-3dd1-4bf4-8241-42c1337f9e0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\rctuh\\\\Desktop\\\\minor projects\\\\Vgg16_Image_Classifier\\\\cats_and_dogs\\\\cats_and_dogs'"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GrMESrVvmGO_",
        "outputId": "0e735b74-b71d-444f-b25f-a9b2f453a66f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\rctuh\\\\Desktop\\\\minor projects\\\\Vgg16_Image_Classifier\\\\cats_and_dogs\\\\cats_and_dogs'"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os \n",
        "os.chdir(ROOT_PATH)\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path=r\"C:\\Users\\rctuh\\Desktop\\minor projects\\Vgg16_Image_Classifier\\cats_and_dogs\\cats_and_dogs\\train\"\n",
        "test_path=r\"C:\\Users\\rctuh\\Desktop\\minor projects\\Vgg16_Image_Classifier\\cats_and_dogs\\cats_and_dogs\\test\"\n",
        "val_path=r\"C:\\Users\\rctuh\\Desktop\\minor projects\\Vgg16_Image_Classifier\\cats_and_dogs\\cats_and_dogs\\validation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train=[]\n",
        "\n",
        "for folder in os.listdir(train_path):\n",
        "\n",
        "    sub_path=train_path+\"/\"+folder\n",
        "\n",
        "    for img in os.listdir(sub_path):\n",
        "\n",
        "        image_path=sub_path+\"/\"+img\n",
        "\n",
        "        img_arr=cv2.imread(image_path)\n",
        "\n",
        "        img_arr=cv2.resize(img_arr,(224,224))\n",
        "\n",
        "        x_train.append(img_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test=[]\n",
        "\n",
        "for folder in os.listdir(test_path):\n",
        "\n",
        "    sub_path=test_path+\"/\"+folder\n",
        "\n",
        "    for img in os.listdir(sub_path):\n",
        "\n",
        "        image_path=sub_path+\"/\"+img\n",
        "\n",
        "        img_arr=cv2.imread(image_path)\n",
        "\n",
        "        img_arr=cv2.resize(img_arr,(224,224))\n",
        "\n",
        "        x_test.append(img_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_val=[]\n",
        "\n",
        "for folder in os.listdir(val_path):\n",
        "\n",
        "    sub_path=val_path+\"/\"+folder\n",
        "\n",
        "    for img in os.listdir(sub_path):\n",
        "\n",
        "        image_path=sub_path+\"/\"+img\n",
        "\n",
        "        img_arr=cv2.imread(image_path)\n",
        "\n",
        "        img_arr=cv2.resize(img_arr,(224,224))\n",
        "\n",
        "        x_val.append(img_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x=np.array(x_train)\n",
        "test_x=np.array(x_test)\n",
        "val_x=np.array(x_val)\n",
        "train_x=train_x/255.0\n",
        "test_x=test_x/255.0\n",
        "val_x=val_x/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6CUAelKmcFx"
      },
      "source": [
        "#Resizing all images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "GQf8CKghmWnC"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE=[224,224]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvFgPy51mzEI"
      },
      "source": [
        "#Importing VGG16 library and add preprocesing layer in front of the VGG16.\n",
        "#Here we will se ImageNet weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9X85mZNm-V-",
        "outputId": "8433194a-55c9-4d2d-b1d0-a8502dba6fc3"
      },
      "outputs": [],
      "source": [
        "vgg16=VGG16(input_shape=IMAGE_SIZE+[3],weights='imagenet',include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku6bOWfgr1p1"
      },
      "source": [
        "include_top= False means that we will cut off the last dense layer of pre-trained model of VGG16 as it was trained for 1000 output categories of imagenet whereas we only have 3 categories. We also cut off the 1st layer because the input image and the size can be of my choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaNaj-pknMDp",
        "outputId": "68d37dfe-c113-42a2-b52e-7c3925912422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x00000247609068C0>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000024760904EE0>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000024760907BE0>\n",
            "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000002475F88E590>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000247608F36A0>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002475FEA9600>\n",
            "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x00000247609539A0>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002475FE8CBE0>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000024771B46770>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002475FE6BEE0>\n",
            "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x00000247730277C0>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000247730251E0>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000024774693340>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000024774693940>\n",
            "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x0000024760790AF0>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000247607AB070>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000247606F1480>\n",
            "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000247606F3E50>\n",
            "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000002476070FA30>\n"
          ]
        }
      ],
      "source": [
        "for layers in vgg16.layers:\n",
        "  print(layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqPR4-3ordR7"
      },
      "source": [
        "###Setting layer.trainable to False moves all the layer's weights from trainable to non-trainable. This is called \"freezing\" the layer: the state of a frozen layer won't be updated during training (either when training with fit() or when training with any custom loop that relies on trainable_weights to apply gradient updates)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "inkzdXgJph_h"
      },
      "outputs": [],
      "source": [
        "for layer in vgg16.layers:\n",
        "  layer.trainable=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS7XhocSprfD",
        "outputId": "b3bedd79-e843-4dd7-fd04-c767e14a0be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_5 False\n",
            "block1_conv1 False\n",
            "block1_conv2 False\n",
            "block1_pool False\n",
            "block2_conv1 False\n",
            "block2_conv2 False\n",
            "block2_pool False\n",
            "block3_conv1 False\n",
            "block3_conv2 False\n",
            "block3_conv3 False\n",
            "block3_pool False\n",
            "block4_conv1 False\n",
            "block4_conv2 False\n",
            "block4_conv3 False\n",
            "block4_pool False\n",
            "block5_conv1 False\n",
            "block5_conv2 False\n",
            "block5_conv3 False\n",
            "block5_pool False\n"
          ]
        }
      ],
      "source": [
        "for layer in vgg16.layers:\n",
        "  print(layer.name,layer.trainable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lVdF3pSp7qB"
      },
      "source": [
        "#Vgg16 Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc3WdCr4p0-U",
        "outputId": "37020d40-44f6-4df2-ac4a-876bf0c494b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vgg16.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRq7gMh1sAfU"
      },
      "source": [
        "#Building the Final Model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "0qtQZ54qp7LJ"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(vgg16)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dense(2,activation='Softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nehx2SYZsTRI"
      },
      "source": [
        "#View the structure of final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPiRE1eksZgx",
        "outputId": "3d7fc6b9-a10d-4f05-a982-f09fd0e6f872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               6422784   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,137,986\n",
            "Trainable params: 6,423,298\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvgc8z2fsfVA"
      },
      "source": [
        "#Specifying the Cost and Optimization Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "6GQA50IRsjeV"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics='accuracy'\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Normalization of values to 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxw13OaltQFg"
      },
      "source": [
        "#Using the Image Data Generator to import images from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "djJoLvKUtV43"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen= ImageDataGenerator(rescale=1./255,\n",
        "                                  shear_range =0.2,\n",
        "                                  zoom_range=0.2,\n",
        "                                  horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen =  ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6wW3vaPuxFp",
        "outputId": "5e81cb44-683a-4e53-f952-e74ac493caff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\rctuh\\\\Desktop\\\\minor projects\\\\Vgg16_Image_Classifier\\\\cats_and_dogs\\\\cats_and_dogs'"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xj23Bmgu3F6",
        "outputId": "f6a4e998-e070-491f-f3c8-b585e9039177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "training_set=train_datagen.flow_from_directory(r'C:\\Users\\rctuh\\Desktop\\minor projects\\Vgg16_Image_Classifier\\cats_and_dogs\\cats_and_dogs\\train',\n",
        "                                               target_size=(224,224),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode='categorical')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFGDqWePva-i",
        "outputId": "d5525a17-15ce-445f-b7cd-f69973080578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_set=test_datagen.flow_from_directory(r'C:\\Users\\rctuh\\Desktop\\minor projects\\Vgg16_Image_Classifier\\cats_and_dogs\\cats_and_dogs\\test',\n",
        "                                               target_size=(224,224),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode='categorical'\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 50 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "val_set=val_datagen.flow_from_directory(r'C:\\Users\\rctuh\\Desktop\\minor projects\\Vgg16_Image_Classifier\\cats_and_dogs\\cats_and_dogs\\validation',\n",
        "                                               target_size=(224,224),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode='categorical'\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_y=training_set.classes\n",
        "test_y=test_set.classes\n",
        "val_y=val_set.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((2000,), (1000,), (50,))"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set.class_indices\n",
        "train_y.shape,test_y.shape,val_y.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Early stopping to avoid overfitting of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om06vOGavf5h"
      },
      "source": [
        "#Model Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_mK8tpyvhEc",
        "outputId": "54275fc3-5b9f-4275-de13-4543e858b591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 148s 2s/step - loss: 0.9189 - accuracy: 0.7885 - val_loss: 0.1605 - val_accuracy: 0.9400\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 151s 2s/step - loss: 0.2227 - accuracy: 0.9050 - val_loss: 0.1992 - val_accuracy: 0.9400\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 145s 2s/step - loss: 0.1768 - accuracy: 0.9335 - val_loss: 0.1470 - val_accuracy: 0.9600\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 153s 2s/step - loss: 0.1413 - accuracy: 0.9395 - val_loss: 0.2626 - val_accuracy: 0.9200\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 148s 2s/step - loss: 0.1549 - accuracy: 0.9400 - val_loss: 0.4678 - val_accuracy: 0.9000\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 144s 2s/step - loss: 0.1048 - accuracy: 0.9570 - val_loss: 0.3226 - val_accuracy: 0.9200\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 145s 2s/step - loss: 0.0957 - accuracy: 0.9640 - val_loss: 0.2770 - val_accuracy: 0.9000\n",
            "Epoch 8/10\n",
            "55/63 [=========================>....] - ETA: 19s - loss: 0.0712 - accuracy: 0.9759"
          ]
        }
      ],
      "source": [
        "r= model.fit(\n",
        "    training_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop],\n",
        "    steps_per_epoch=len(training_set),\n",
        "    validation_steps=len(val_set)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rAligah5awy"
      },
      "source": [
        "#Plotting the Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "040VS1XX5foz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKOuu9Xk5kmf"
      },
      "outputs": [],
      "source": [
        "plt.plot(r.history['loss'], label='Train Loss')\n",
        "plt.plot(r.history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39QZe6GCDJb-"
      },
      "source": [
        "#Plotting Accuracy \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91mmHaeADMdJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(r.history['accuracy'], label='Train Acc')\n",
        "plt.plot(r.history['val_accuracy'], label='Val Acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yStFlwRWHNK2"
      },
      "source": [
        "#Saving the File as .h5 Type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDaGubcJHQ_d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model \n",
        "model.save('model_vgg16.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF0z6LvHHcTd"
      },
      "source": [
        "#Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIU8dAvcHeYJ"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(test_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOCrPkdAHjgz"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXT2XuynHmRa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "y_pred=np.argmax(y_pred,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w37eiO1dHtLb"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnPrk5IMHt-K"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neSsdoFYH1Po"
      },
      "outputs": [],
      "source": [
        "model=load_model('model_vgg16.h5')\n",
        "img_path=r'C:\\Users\\rctuh\\Desktop\\minor projects\\Vgg16_Image_Classifier\\cats_and_dogs\\cats_and_dogs\\validation\\dogs\\dog.2012.jpg'\n",
        "img=image.load_img(img_path,target_size=(224,224))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBrjUrdMISG7"
      },
      "outputs": [],
      "source": [
        "x=image.img_to_array(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITsZQM7LIXA_"
      },
      "outputs": [],
      "source": [
        "Z=plt.imread(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzzDXmcyIevS"
      },
      "outputs": [],
      "source": [
        "plt.imshow(Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s27coclVIj2o"
      },
      "outputs": [],
      "source": [
        "x.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x=x/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w6wf6MoInPc"
      },
      "outputs": [],
      "source": [
        "from keras.applications.vgg16 import preprocess_input "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0690VJUIs2L"
      },
      "outputs": [],
      "source": [
        "x=np.expand_dims(x,axis=0)\n",
        "img_data=preprocess_input(x)\n",
        "img_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EddcOQ_jI3XU"
      },
      "outputs": [],
      "source": [
        "model.predict(img_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w1bTK6xI-t6"
      },
      "outputs": [],
      "source": [
        "result=np.argmax(model.predict(img_data),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmNU4OYHJFj8"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFNHfYeyJGNc"
      },
      "outputs": [],
      "source": [
        "if result[0] == 1:\n",
        "  prediction = \"The Given Image is of a Dog\"\n",
        "  print(prediction)\n",
        "else:\n",
        "  prediction=\"The Given Image is of a cat\"\n",
        "  print(prediction)\n",
        "  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Losses and Accuracy \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " from sklearn.metrics import accuracy_score\n",
        " from sklearn.metrics import f1_score\n",
        " from sklearn.metrics import roc_auc_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1_score(test_y,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_score(test_y, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roc_auc_score(test_y,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_pred,test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(confusion_matrix(y_pred,test_y))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNMZUNyR0K17otKWmEkfjNR",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
